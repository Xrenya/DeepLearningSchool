{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[homework]embeddings.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot3c4fjZwC4T"
      },
      "source": [
        "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
        "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2JdzEXmwRU5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYtJxkhKpYK2"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "Привет! В этом домашнем задании мы с помощью эмбеддингов решим задачу семантической классификации твитов.\n",
        "\n",
        "Для этого мы воспользуемся предобученными эмбеддингами word2vec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBOdoFS8AdpP"
      },
      "source": [
        "Для начала скачаем датасет для семантической классификации твитов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXjhtsfF_gBK"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1eE1FiUkXkcbw0McId4i7qY-L8hH-_Qph&export=download\n",
        "!unzip archive.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh6wW-K53Mle"
      },
      "source": [
        "Импортируем нужные библиотеки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Y5CHRm6NFe"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import nltk\n",
        "import gensim\n",
        "import gensim.downloader as api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Lb0wbESrgQ"
      },
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.random.manual_seed(42)\n",
        "torch.cuda.random.manual_seed(42)\n",
        "torch.cuda.random.manual_seed_all(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_Wv-4bu83Fl"
      },
      "source": [
        "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"latin\", header=None, names=[\"emotion\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY1pvYDS3Yuj"
      },
      "source": [
        "Посмотрим на данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jST2tjgjCTWD"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhbR5JJyA2VW"
      },
      "source": [
        "Выведем несколько примеров твитов, чтобы понимать, с чем мы имеем дело:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCBwe0wR83C2"
      },
      "source": [
        "examples = data[\"text\"].sample(10)\n",
        "print(\"\\n\".join(examples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvcYW8aX3mKt"
      },
      "source": [
        "Как вилим, тексты твитов очень \"грязные\". Нужно предобработать датасет, прежде чем строить для него модель классификации.\n",
        "\n",
        "Чтобы сравнивать различные методы обработки текста/модели/прочее, разделим датасет на dev(для обучения модели) и test(для получения качества модели)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8hUK-jnQg6O"
      },
      "source": [
        "indexes = np.arange(data.shape[0])\n",
        "np.random.shuffle(indexes)\n",
        "dev_size = math.ceil(data.shape[0] * 0.8)\n",
        "\n",
        "dev_indexes = indexes[:dev_size]\n",
        "test_indexes = indexes[dev_size:]\n",
        "\n",
        "dev_data = data.iloc[dev_indexes]\n",
        "test_data = data.iloc[test_indexes]\n",
        "\n",
        "dev_data.reset_index(drop=True, inplace=True)\n",
        "test_data.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ivcpeFoCnZA"
      },
      "source": [
        "## Обработка текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df4nca285Dar"
      },
      "source": [
        "Токенизируем текст, избавимся от знаков пунктуации и выкинем все слова, состоящие менее чем из 4 букв:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsNHNDES9ZVF"
      },
      "source": [
        "tokenizer = nltk.WordPunctTokenizer()\n",
        "line = tokenizer.tokenize(dev_data[\"text\"][0].lower())\n",
        "print(\" \".join(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcBS_u_hTuxp"
      },
      "source": [
        "filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
        "print(\" \".join(filtered_line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eity0feqo6aC"
      },
      "source": [
        "filtered_line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuFmlXkC6E7X"
      },
      "source": [
        "Загрузим предобученную модель эмбеддингов. \n",
        "\n",
        "Если хотите, можно попробовать другую. Полный список можно найти здесь: https://github.com/RaRe-Technologies/gensim-data.\n",
        "\n",
        "Данная модель выдает эмбеддинги для **слов**. Строить по эмбеддингам слов эмбеддинги предложений мы будем ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cACJpje2T5bc"
      },
      "source": [
        "word2vec = api.load(\"word2vec-google-news-300\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NafmYHrkT5YD"
      },
      "source": [
        "emb_line = [word2vec.get_vector(w) for w in filtered_line if w in word2vec]\n",
        "print(sum(emb_line).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTS6LCkd6_E7"
      },
      "source": [
        "Нормализуем эмбеддинги, прежде чем обучать на них сеть. \n",
        "(наверное, вы помните, что нейронные сети гораздо лучше обучаются на нормализованных данных)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PyLTZ6xf3Oq"
      },
      "source": [
        "mean = np.mean(word2vec.vectors, 0)\n",
        "std = np.std(word2vec.vectors, 0)\n",
        "norm_emb_line = [(word2vec.get_vector(w) - mean) / std for w in filtered_line if w in word2vec and len(w) > 3]\n",
        "print(sum(norm_emb_line).shape)\n",
        "print([all(norm_emb_line[i] == emb_line[i]) for i in range(len(emb_line))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkTxIwe2pGsa"
      },
      "source": [
        "norm_emb_line[0][:5], emb_line[0][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vm6Ppd7Ubw"
      },
      "source": [
        "Сделаем датасет, который будет по запросу возвращать подготовленные данные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4eZajF7pZ1X"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "\n",
        "class TwitterDataset(Dataset):  \n",
        "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\n",
        "        \n",
        "        self.data = data\n",
        "\n",
        "        self.feature_column = feature_column\n",
        "        self.target_column = target_column\n",
        "\n",
        "        self.word2vec = word2vec\n",
        "\n",
        "        self.label2num = lambda label: 0 if label == 0 else 1\n",
        "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
        "        self.std = np.std(word2vec.vectors, axis=0)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.data[self.feature_column][item]\n",
        "        label = self.label2num(self.data[self.target_column][item])\n",
        "\n",
        "        tokens = self.get_tokens_(text)\n",
        "        embeddings = self.get_embeddings_(tokens)\n",
        "\n",
        "        return {\"feature\": embeddings, \"target\": label}\n",
        "\n",
        "    def get_tokens_(self, text):\n",
        "        # Получи все токены из текста и профильтруй их\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokens = [w for w in tokens if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
        "        tokens = [w for w in tokens if w in word2vec]\n",
        "        return tokens\n",
        "\n",
        "    def get_embeddings_(self, tokens):\n",
        "        # Получи эмбеддинги слов и усредни их\n",
        "        embeddings = [(self.word2vec.get_vector(token) - self.mean) / self.std for token in (tokens)]\n",
        "        if len(embeddings) == 0:\n",
        "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
        "        else:\n",
        "            embeddings = np.array(embeddings)\n",
        "            if len(embeddings.shape) == 1:\n",
        "                embeddings = embeddings.reshape(-1, 1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSENCcuFL7Um"
      },
      "source": [
        "a = [word2vec.get_vector(token).shape for token in [\"cat\"]]\n",
        "np.array(a).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_oZc6_VMTQn"
      },
      "source": [
        "a = np.array(a)\n",
        "print(len(a))\n",
        "if len(a.shape) == 1:\n",
        "    print(a.reshape(-1, 1).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZJpttbXpZyz"
      },
      "source": [
        "dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y6L1RJkik8S"
      },
      "source": [
        "for data in dev:\n",
        "    print(data[\"feature\"])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr-aetH0_LH1"
      },
      "source": [
        "Отлично, мы готовы с помощью эмбеддингов слов превращать твиты в векторы и обучать нейронную сеть.\n",
        "\n",
        "Превращать твиты в векторы, используя эмбеддинги слов, можно несколькими способами. А именно такими:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AhHrWa196Yc"
      },
      "source": [
        "## Average embedding (2 балла)\n",
        "---\n",
        "Это самый простой вариант, как получить вектор предложения, используя векторные представления слов в предложении. А именно: вектор предложения есть средний вектор всех слов в предлоежнии (которые остались после токенизации и удаления коротких слов, конечно). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScdokSW-994t"
      },
      "source": [
        "indexes = np.arange(len(dev))\n",
        "np.random.shuffle(indexes)\n",
        "example_indexes = indexes[::1000]\n",
        "\n",
        "examples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n",
        "            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n",
        "print(len(examples[\"features\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eQhu8lprUqc"
      },
      "source": [
        "examples[\"features\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yGQ_lOx_1NL"
      },
      "source": [
        "Давайте сделаем визуализацию полученных векторов твитов тренировочного (dev) датасета. Так мы увидим, насколько хорошо твиты с разными target значениями отделяются друг от друга, т.е. насколько хорошо усреднение эмбеддингов слов предложения передает информацию о предложении."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZwFksd_8uYO"
      },
      "source": [
        "Для визуализации векторов надо получить их проекцию на плоскость. Сделаем это с помощью `PCA`. Если хотите, можете вместо PCA использовать TSNE: так у вас получится более точная проекция на плоскость (а значит, более информативная, т.е. отражающая реальное положение векторов твитов в пространстве). Но TSNE будет работать намного дольше."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKFZRSHdtIac"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "examples[\"transformed_features\"] = pca.fit_transform(examples[\"features\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szEOWdiNtIX8"
      },
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: pl.show(fig)\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OONK8ldtIWe"
      },
      "source": [
        "draw_vectors(\n",
        "    examples[\"transformed_features\"][:, 0], \n",
        "    examples[\"transformed_features\"][:, 1], \n",
        "    color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fNF6LRQ9MPI"
      },
      "source": [
        "Скорее всего, на визуализации нет четкого разделения твитов между классами. Это значит, что по полученным нами векторам твитов не так-то просто определить, к какому классу твит пренадлежит. Значит, обычный линейный классификатор не очень хорошо справится с задачей. Надо будет делать глубокую (хотя бы два слоя) нейронную сеть.\n",
        "\n",
        "Подготовим загрузчики данных.\n",
        "Усреднее векторов будем делать в \"батчевалке\"(`collate_fn`). Она используется для того, чтобы собирать из данных `torch.Tensor` батчи, которые можно отправлять в модель.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1XapsADtITv"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "batch_size = 1024\n",
        "num_workers = 4\n",
        "\n",
        "def average_emb(batch):\n",
        "    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n",
        "    targets = [b[\"target\"] for b in batch]\n",
        "\n",
        "    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
        "\n",
        "\n",
        "train_size = math.ceil(len(dev) * 0.8)\n",
        "\n",
        "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
        "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcKm34lKs-PN"
      },
      "source": [
        "for data in train_loader:\n",
        "    print(data['features'].shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-zs0WEK-Vkt"
      },
      "source": [
        "Определим функции для тренировки и теста модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U--T2Gjw1r27"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def training(model, optimizer, criterion, train_loader, epoch, device=\"cpu\"):\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {e + 1}. Train Loss: {0}\")\n",
        "    model.train()\n",
        "    for batch in pbar:\n",
        "        features = batch[\"features\"].to(device)\n",
        "        targets = batch[\"targets\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Получи предсказания модели\n",
        "        preds = model.forward(features)\n",
        "        loss = criterion(preds, targets) # Посчитай лосс\n",
        "        # Обнови параметры модели\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(f\"Epoch {e + 1}. Train Loss: {loss:.4}\")\n",
        "    \n",
        "\n",
        "def testing(model, criterion, test_loader, device=\"cpu\"):\n",
        "    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n",
        "    mean_loss = 0\n",
        "    mean_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            features = batch[\"features\"].to(device)\n",
        "            targets = batch[\"targets\"].to(device)\n",
        "\n",
        "            # Получи предсказания модели\n",
        "            preds = model.forward(features)\n",
        "            #print(torch.max(preds, 1)[1])\n",
        "            #print(torch.max(preds, dim=1))\n",
        "            loss = criterion(preds, targets)\n",
        "            #print(preds) # Посчитай лосс\n",
        "            acc = sum(torch.argmax(preds, dim=1) == targets) # Посчитай точность модели\n",
        "\n",
        "            mean_loss += loss.item()\n",
        "            #print(acc.item(), acc.item() / len(targets))\n",
        "            mean_acc += acc.item() / len(targets)\n",
        "\n",
        "            pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc.item() / len(targets):.4}\")\n",
        "\n",
        "    pbar.set_description(f\"Test Loss: {mean_loss / len(test_loader):.4}, Test Acc: {mean_acc / len(test_loader):.4}\")\n",
        "\n",
        "    return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVg_XBBb-YBH"
      },
      "source": [
        "Создадим модель, оптимизатор и целевую функцию. Вы можете сами выбрать количество слоев в нейронной сети, ваш любимый оптимизатор и целевую функцию.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdqP0hnQuMs-"
      },
      "source": [
        "import torch.nn as nn\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, hidden // 2)\n",
        "        self.linear3 = nn.Linear(hidden // 2, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.linear2(out)\n",
        "        return self.linear3(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBoZ4F3Fx1Hm"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "# Не забудь поиграться с параметрами ;)\n",
        "vector_size = dev.word2vec.vector_size\n",
        "num_classes = 2\n",
        "lr = 1e-2\n",
        "num_epochs = 1\n",
        "\n",
        "model = Classifier(300, 150, num_classes)# Твоя модель\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Твой лосс\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr) # Твой оптимайзер"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AitU8AR-zBj"
      },
      "source": [
        "Наконец, обучим модель и протестируем её.\n",
        "\n",
        "После каждой эпохи будем проверять качество модели на валидационной части датасета. Если метрика стала лучше, будем сохранять модель. **Подумайте, какая метрика (точность или лосс) будет лучше работать в этой задаче?** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKhk71Pmx1F1"
      },
      "source": [
        "best_metric = np.inf\n",
        "for e in range(num_epochs):\n",
        "    training(model, optimizer, criterion, train_loader, e, device)\n",
        "    log = testing(model, criterion, valid_loader, device)\n",
        "    if log[\"Test Loss\"] < best_metric:\n",
        "        torch.save(model.state_dict(), \"model.pt\")\n",
        "        best_metric = log[\"Test Loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di4dGwD4x1Dt"
      },
      "source": [
        "test_loader = DataLoader(\n",
        "    TwitterDataset(test_data, \"text\", \"emotion\", word2vec), \n",
        "    batch_size=batch_size, \n",
        "    num_workers=num_workers, \n",
        "    shuffle=False,\n",
        "    drop_last=False, \n",
        "    collate_fn=average_emb)\n",
        "\n",
        "model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
        "\n",
        "print(testing(model, criterion, test_loader, device=device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRvzpldHSAu0"
      },
      "source": [
        "## Embeddings for unknown words (8 баллов)\n",
        "\n",
        "Пока что использовалась не вся информация из текста. Часть информации фильтровалось – если слова не было в словаре эмбеддингов, то мы просто превращали слово в нулевой вектор. Хочется использовать информацию по-максимуму. Поэтому рассмотрим другие способы обработки слов, которых нет в словаре. А именно:\n",
        "\n",
        "- Для каждого незнакомого слова будем запоминать его контекст(слова слева и справа от этого слова). Эмбеддингом нашего незнакомого слова будет сумма эмбеддингов всех слов из его контекста. (4 балла)\n",
        "- Для каждого слова текста получим его эмбеддинг из Tfidf с помощью ```TfidfVectorizer``` из [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer). Итоговым эмбеддингом для каждого слова будет сумма двух эмбеддингов: предобученного и Tfidf-ного. Для слов, которых нет в словаре предобученных эмбеддингов, результирующий эмбеддинг будет просто полученный из Tfidf. (4 балла)\n",
        "\n",
        "Реализуйте оба варианта **ниже**. Напишите, какой способ сработал лучше и ваши мысли, почему так получилось."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxhEpKalU1UQ"
      },
      "source": [
        "class TwitterDatasetAverage(Dataset):  \n",
        "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\n",
        "        \n",
        "        self.data = data\n",
        "\n",
        "        self.feature_column = feature_column\n",
        "        self.target_column = target_column\n",
        "\n",
        "        self.word2vec = word2vec\n",
        "\n",
        "        self.label2num = lambda label: 0 if label == 0 else 1\n",
        "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
        "        self.std = np.std(word2vec.vectors, axis=0)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.data[self.feature_column][item]\n",
        "        label = self.label2num(self.data[self.target_column][item])\n",
        "\n",
        "        tokens = self.get_tokens_(text)\n",
        "        embeddings = self.get_embeddings_(tokens)\n",
        "\n",
        "        return {\"feature\": embeddings, \"target\": label}\n",
        "\n",
        "    def get_tokens_(self, text):\n",
        "        # Получи все токены из текста и профильтруй их\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokens = [w for w in tokens if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
        "        return tokens\n",
        "\n",
        "    def get_embeddings_(self, tokens):\n",
        "        # Получи эмбеддинги слов и усредни их\n",
        "        embeddings = []\n",
        "        window_size = 1\n",
        "        for idx, token in enumerate(tokens):\n",
        "            if token in self.word2vec:\n",
        "                embeddings.append((self.word2vec.get_vector(token) - self.mean) / self.std)\n",
        "            else:\n",
        "                start_idx = max(0, idx - window_size)\n",
        "                end_idx = min(len(tokens), idx + window_size)\n",
        "                pos_in_window = idx\n",
        "                if idx - pos_in_window < 0:\n",
        "                    pos_in_window += idx - window_size\n",
        "                collector = np.zeros((1, self.word2vec.vector_size)).flatten()\n",
        "                flag_1 = True\n",
        "                flag_2 = True\n",
        "                cnt = 0\n",
        "                while flag_1:\n",
        "                    if tokens[start_idx] in word2vec and start_idx > 0 and start_idx != pos_in_window:\n",
        "                        collector += (self.word2vec.get_vector(tokens[start_idx]) - self.mean) / self.std \n",
        "                        flag_1 = False\n",
        "                        cnt += 1\n",
        "                    elif start_idx > 0:\n",
        "                        start_idx -= 1\n",
        "                    else:\n",
        "                        collector += np.zeros((1, self.word2vec.vector_size)).flatten()\n",
        "                        flag_1 = False\n",
        "                while flag_2:\n",
        "                    if end_idx < len(tokens) and end_idx != pos_in_window and tokens[end_idx] in word2vec:\n",
        "                        collector += (self.word2vec.get_vector(tokens[end_idx]) - self.mean) / self.std \n",
        "                        flag_2 = False\n",
        "                        cnt += 1\n",
        "                    elif end_idx < len(tokens) - 1:\n",
        "                        end_idx += 1\n",
        "                    else:\n",
        "                        collector += np.zeros((1, self.word2vec.vector_size)).flatten()\n",
        "                        flag_2 = False\n",
        "                if cnt == 0:\n",
        "                    avg_emg = np.zeros((1, self.word2vec.vector_size)).flatten()\n",
        "                else:\n",
        "                    avg_emg = collector / cnt\n",
        "                embeddings.append(avg_emg)\n",
        "        if len(embeddings) == 0:\n",
        "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
        "        else:\n",
        "            embeddings = np.array(embeddings)\n",
        "            if len(embeddings.shape) == 1:\n",
        "                embeddings = embeddings.reshape(-1, 1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UZ5eTqkLFZ7"
      },
      "source": [
        "dev = TwitterDatasetAverage(dev_data, \"text\", \"emotion\", word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYZP9MUfLuqc"
      },
      "source": [
        "for d in dev:\n",
        "    print(d[\"feature\"].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8SDHG7kK7z3"
      },
      "source": [
        "batch_size = 1024\n",
        "num_workers = 4\n",
        "\n",
        "def average_emb(batch):\n",
        "    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n",
        "    targets = [b[\"target\"] for b in batch]\n",
        "\n",
        "    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
        "\n",
        "\n",
        "train_size = math.ceil(len(dev) * 0.8)\n",
        "\n",
        "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
        "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwnyLA_CLI6N"
      },
      "source": [
        "vector_size = dev.word2vec.vector_size\n",
        "num_classes = 2\n",
        "lr = 1e-2\n",
        "num_epochs = 1\n",
        "\n",
        "model_avg = Classifier(300, 150, num_classes)# Твоя модель\n",
        "model_avg = model_avg.to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Твой лосс\n",
        "optimizer = torch.optim.Adam(model_avg.parameters(), lr=lr) # Твой оптимайзер\n",
        "best_metric = np.inf\n",
        "for e in range(num_epochs):\n",
        "    training(model_avg, optimizer, criterion, train_loader, e, device)\n",
        "    log = testing(model_avg, criterion, valid_loader, device)\n",
        "    if log[\"Test Loss\"] < best_metric:\n",
        "        torch.save(model_avg.state_dict(), \"model_avg.pt\")\n",
        "        best_metric = log[\"Test Loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5jCtEVoRmIF"
      },
      "source": [
        "test_loader = DataLoader(\n",
        "    TwitterDatasetAverage(test_data, \"text\", \"emotion\", word2vec), \n",
        "    batch_size=batch_size, \n",
        "    num_workers=num_workers, \n",
        "    shuffle=False,\n",
        "    drop_last=False, \n",
        "    collate_fn=average_emb)\n",
        "\n",
        "model.load_state_dict(torch.load(\"model_avg.pt\", map_location=device))\n",
        "\n",
        "print(testing(model, criterion, test_loader, device=device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kMbxTn0RmYu"
      },
      "source": [
        "#0.719854482827476"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrklAeZWLaWm"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkDQv_TmWpHS"
      },
      "source": [
        "class TwitterDatasetTfidf(Dataset):  \n",
        "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec, tfidf):\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\n",
        "        \n",
        "        self.data = data\n",
        "\n",
        "        self.feature_column = feature_column\n",
        "        self.target_column = target_column\n",
        "\n",
        "        self.word2vec = word2vec\n",
        "        self.tfidf = tfidf\n",
        "\n",
        "        self.label2num = lambda label: 0 if label == 0 else 1\n",
        "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
        "        self.std = np.std(word2vec.vectors, axis=0)\n",
        "        self._tfidf()\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.data[self.feature_column][item]\n",
        "        label = self.label2num(self.data[self.target_column][item])\n",
        "        tfidf_emdeddings = self.tfidf.transform([text]).toarray().flatten()\n",
        "        tokens = self.get_tokens_(text)\n",
        "        embeddings = self.get_embeddings_(tokens) + tfidf_emdeddings\n",
        "\n",
        "        return {\"feature\": embeddings, \"target\": label}\n",
        "\n",
        "    def _tfidf(self):\n",
        "        text = [self.data[self.feature_column][item] for item in \n",
        "                range(len(self.data[self.feature_column]))]\n",
        "        self.tfidf.fit_transform(text)\n",
        "\n",
        "    def get_tokens_(self, text):\n",
        "        # Получи все токены из текста и профильтруй их\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokens = [w for w in tokens if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
        "        tokens = [w for w in tokens if w in word2vec]\n",
        "        return tokens\n",
        "\n",
        "    def get_embeddings_(self, tokens):\n",
        "        # Получи эмбеддинги слов и усредни их\n",
        "        embeddings = [(self.word2vec.get_vector(token) - self.mean) / self.std for token in (tokens)]\n",
        "        if len(embeddings) == 0:\n",
        "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
        "        else:\n",
        "            embeddings = np.array(embeddings)\n",
        "            if len(embeddings.shape) == 1:\n",
        "                embeddings = embeddings.reshape(-1, 1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X2s2gJNXxZu"
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features=300)\n",
        "dev = TwitterDatasetTfidf(dev_data, \"text\", \"emotion\", word2vec, vectorizer)\n",
        "\n",
        "batch_size = 1024\n",
        "num_workers = 4\n",
        "\n",
        "def average_emb(batch):\n",
        "    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n",
        "    targets = [b[\"target\"] for b in batch]\n",
        "\n",
        "    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
        "\n",
        "\n",
        "train_size = math.ceil(len(dev) * 0.8)\n",
        "\n",
        "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
        "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLxlAij6X4w3"
      },
      "source": [
        "#dev.tfidf.get_feature_names()\n",
        "for d in dev:\n",
        "    print(d[\"feature\"].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8eoR1abiIP"
      },
      "source": [
        "dev.tfidf.transform([\"freedom comes at the cost\"]).toarray().flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ-dBOHLchHN"
      },
      "source": [
        "dict(zip(dev.tfidf.get_feature_names(), list(dev.tfidf.idf_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow5Ce5B3g5N3"
      },
      "source": [
        "dev.tfidf.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2eyQSkzhHkH"
      },
      "source": [
        "vector_size = dev.word2vec.vector_size\n",
        "num_classes = 2\n",
        "lr = 1e-2\n",
        "num_epochs = 1\n",
        "\n",
        "model = Classifier(300, 150, num_classes)# Твоя модель\n",
        "model_avg = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Твой лосс\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr) # Твой оптимайзер\n",
        "best_metric = np.inf\n",
        "for e in range(num_epochs):\n",
        "    training(model, optimizer, criterion, train_loader, e, device)\n",
        "    log = testing(model, criterion, valid_loader, device)\n",
        "    if log[\"Test Loss\"] < best_metric:\n",
        "        torch.save(model.state_dict(), \"model_tfidf.pt\")\n",
        "        best_metric = log[\"Test Loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRF7WPvti3RW"
      },
      "source": [
        "test_loader = DataLoader(\n",
        "    TwitterDatasetTfidf(test_data, \"text\", \"emotion\", word2vec, vectorizer), \n",
        "    batch_size=batch_size, \n",
        "    num_workers=num_workers, \n",
        "    shuffle=False,\n",
        "    drop_last=False, \n",
        "    collate_fn=average_emb)\n",
        "\n",
        "model.load_state_dict(torch.load(\"model_tfidf.pt\", map_location=device))\n",
        "\n",
        "print(testing(model, criterion, test_loader, device=device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ9ddfynm8Ai"
      },
      "source": [
        "class TwitterDatasetTfidf_(Dataset):  \n",
        "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec, tfidf):\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\n",
        "        \n",
        "        self.data = data\n",
        "\n",
        "        self.feature_column = feature_column\n",
        "        self.target_column = target_column\n",
        "\n",
        "        self.word2vec = word2vec\n",
        "        self.tfidf = tfidf\n",
        "\n",
        "        self.label2num = lambda label: 0 if label == 0 else 1\n",
        "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
        "        self.std = np.std(word2vec.vectors, axis=0)\n",
        "        self._tfidf()\n",
        "        self.dictionary = dict(zip(self.tfidf.get_feature_names(), list(self.tfidf.idf_)))\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.data[self.feature_column][item]\n",
        "        label = self.label2num(self.data[self.target_column][item])\n",
        "        tfidf_emdeddings = self.tfidf.transform([text]).toarray().flatten()\n",
        "        tokens = self.get_tokens_(text)\n",
        "        embeddings = self.get_embeddings_(tokens) + tfidf_emdeddings\n",
        "\n",
        "        return {\"feature\": embeddings, \"target\": label}\n",
        "\n",
        "    def _tfidf(self):\n",
        "        text = [self.data[self.feature_column][item] for item in \n",
        "                range(len(self.data[self.feature_column]))]\n",
        "        self.tfidf.fit_transform(text)\n",
        "\n",
        "    def get_tokens_(self, text):\n",
        "        # Получи все токены из текста и профильтруй их\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokens = [w for w in tokens if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
        "        tokens = [w for w in tokens if w in word2vec]\n",
        "        return tokens\n",
        "\n",
        "    def get_embeddings_(self, tokens):\n",
        "        # Получи эмбеддинги слов и усредни их\n",
        "        embeddings = [(self.dictionary.get(token, 1) * self.word2vec.get_vector(token) - self.mean) / self.std for token in (tokens)]\n",
        "        if len(embeddings) == 0:\n",
        "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
        "        else:\n",
        "            embeddings = np.array(embeddings)\n",
        "            if len(embeddings.shape) == 1:\n",
        "                embeddings = embeddings.reshape(-1, 1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfeuaBbgpf-1"
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features=300)\n",
        "dev = TwitterDatasetTfidf_(dev_data, \"text\", \"emotion\", word2vec, vectorizer)\n",
        "\n",
        "batch_size = 1024\n",
        "num_workers = 4\n",
        "\n",
        "def average_emb(batch):\n",
        "    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n",
        "    targets = [b[\"target\"] for b in batch]\n",
        "\n",
        "    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
        "\n",
        "\n",
        "train_size = math.ceil(len(dev) * 0.8)\n",
        "\n",
        "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
        "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C4s1C3ipZ2m"
      },
      "source": [
        "vector_size = dev.word2vec.vector_size\n",
        "num_classes = 2\n",
        "lr = 1e-2\n",
        "num_epochs = 1\n",
        "\n",
        "model = Classifier(300, 150, num_classes)# Твоя модель\n",
        "model_avg = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Твой лосс\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr) # Твой оптимайзер\n",
        "best_metric = np.inf\n",
        "for e in range(num_epochs):\n",
        "    training(model, optimizer, criterion, train_loader, e, device)\n",
        "    log = testing(model, criterion, valid_loader, device)\n",
        "    if log[\"Test Loss\"] < best_metric:\n",
        "        torch.save(model.state_dict(), \"model_tfidf.pt\")\n",
        "        best_metric = log[\"Test Loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzGP6CrSuHzb"
      },
      "source": [
        "log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jyg3169pcEN"
      },
      "source": [
        "test_loader = DataLoader(\n",
        "    TwitterDatasetTfidf_(test_data, \"text\", \"emotion\", word2vec, vectorizer), \n",
        "    batch_size=batch_size, \n",
        "    num_workers=num_workers, \n",
        "    shuffle=False,\n",
        "    drop_last=False, \n",
        "    collate_fn=average_emb)\n",
        "\n",
        "model.load_state_dict(torch.load(\"model_tfidf.pt\", map_location=device))\n",
        "\n",
        "print(testing(model, criterion, test_loader, device=device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WJdnhSXuJ1E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}